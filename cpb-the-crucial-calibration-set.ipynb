{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[],"dockerImageVersionId":30919,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"source":"<a href=\"https://www.kaggle.com/code/elainedazzio/cpb-the-crucial-calibration-set?scriptVersionId=261336036\" target=\"_blank\"><img align=\"left\" alt=\"Kaggle\" title=\"Open in Kaggle\" src=\"https://kaggle.com/static/images/open-in-kaggle.svg\"></a>","metadata":{},"cell_type":"markdown"},{"cell_type":"markdown","source":"\n# Conformal Prediction Basics: The Crucial Calibration Set\n\n**Introduction to Conformal Prediction with Synthetic Regression Data**\n\nHello! This notebook explores how conformal prediction works, using a simple synthetic regression dataset.  Conformal prediction provides a measure of confidence in its predictions, which is crucial in many applications.\n\nWe'll walk through the process step-by-step:\n\n1.  Generating synthetic regression data.\n2.  Training a simple linear regression model.\n3.  Implementing conformal prediction using the calibration set.\n4.  Analyzing how the choice of the calibration set affects the validity of our predictions.\n\nBy the end of this notebook, you'll understand how conformal prediction can be applied to quantify the uncertainty of a model's predictions.\n","metadata":{}},{"cell_type":"code","source":"import numpy as np\nimport matplotlib.pyplot as plt\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.linear_model import LinearRegression","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-22T02:35:18.152203Z","iopub.execute_input":"2025-03-22T02:35:18.152557Z","iopub.status.idle":"2025-03-22T02:35:18.156798Z","shell.execute_reply.started":"2025-03-22T02:35:18.152533Z","shell.execute_reply":"2025-03-22T02:35:18.155932Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# 1. Generate Sample Data (Regression)\nnp.random.seed(0)\nX = np.sort(5 * np.random.rand(100, 1), axis=0)  # 100 samples, 1 feature\ny = np.sin(X).ravel() + np.random.randn(100) * 0.1  # Add some noise\nX = X.astype(np.float32)\ny = y.astype(np.float32)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-22T02:35:18.157939Z","iopub.execute_input":"2025-03-22T02:35:18.158262Z","iopub.status.idle":"2025-03-22T02:35:18.176619Z","shell.execute_reply.started":"2025-03-22T02:35:18.158231Z","shell.execute_reply":"2025-03-22T02:35:18.175962Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"\nHere, we generate a synthetic dataset for a regression problem.  The input feature `X` is a set of 100 points between 0 and 5.  The output `y` is calculated using the sine function, with added random noise to simulate real-world data.  This synthetic data allows us to clearly illustrate the concepts of conformal prediction without the complexities of real-world image data.\n","metadata":{}},{"cell_type":"code","source":"# 2. Split Data into Training, Calibration, and Test Sets\nX_train, X_cal_test, y_train, y_cal_test = train_test_split(X, y, test_size=0.4, random_state=42) # 40% for calibration and test\nX_cal, X_test, y_cal, y_test = train_test_split(X_cal_test, y_cal_test, test_size=0.5, random_state=42) # split the cal_test into half","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-22T02:35:18.178638Z","iopub.execute_input":"2025-03-22T02:35:18.178862Z","iopub.status.idle":"2025-03-22T02:35:18.193912Z","shell.execute_reply.started":"2025-03-22T02:35:18.178844Z","shell.execute_reply":"2025-03-22T02:35:18.193246Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"\nWe split the data into three sets:\n\n* **Training set:** Used to train the machine learning model.\n* **Calibration set:** Used to determine the nonconformity scores, which are essential for creating the prediction intervals.\n* **Test set:** Used to evaluate the performance of the conformal prediction method, including its coverage.\n\nThe calibration set is crucial in conformal prediction. It allows us to estimate the distribution of errors that the model makes, which we then use to construct prediction intervals with a desired level of confidence.\n","metadata":{}},{"cell_type":"code","source":"# 3. Train a Model (Linear Regression)\nmodel = LinearRegression()\nmodel.fit(X_train, y_train)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-22T02:35:18.195246Z","iopub.execute_input":"2025-03-22T02:35:18.195768Z","iopub.status.idle":"2025-03-22T02:35:18.215902Z","shell.execute_reply.started":"2025-03-22T02:35:18.195736Z","shell.execute_reply":"2025-03-22T02:35:18.215Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"\nWe train a simple linear regression model on the training data.  Linear regression is used here for its simplicity, allowing us to focus on the conformal prediction framework.  The same principles can be applied to more complex models.\n","metadata":{}},{"cell_type":"code","source":"# 4. Define Nonconformity Measure (Residual)\ndef nonconformity_measure(y_hat, y):\n    return np.abs(y_hat - y)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-22T02:35:18.216962Z","iopub.execute_input":"2025-03-22T02:35:18.217299Z","iopub.status.idle":"2025-03-22T02:35:18.2325Z","shell.execute_reply.started":"2025-03-22T02:35:18.217275Z","shell.execute_reply":"2025-03-22T02:35:18.231552Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"\nThe nonconformity measure quantifies how \"different\" a prediction is from the true value.  In this case, we use the absolute value of the residual (the difference between the predicted and actual values).  Other nonconformity measures can be used, depending on the specific problem and model.\n","metadata":{}},{"cell_type":"code","source":"# 5. Calculate Nonconformity Scores on Calibration Set\ny_cal_pred = model.predict(X_cal)\ncalibration_scores = nonconformity_measure(y_cal_pred, y_cal)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-22T02:35:18.233559Z","iopub.execute_input":"2025-03-22T02:35:18.233858Z","iopub.status.idle":"2025-03-22T02:35:18.249649Z","shell.execute_reply.started":"2025-03-22T02:35:18.233831Z","shell.execute_reply":"2025-03-22T02:35:18.249017Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"\nWe calculate the nonconformity scores for the calibration set.  These scores represent how well the model's predictions on the calibration set match the actual values.  These scores are crucial for determining the quantile in the next step.\n","metadata":{}},{"cell_type":"code","source":"# 6. Determine Quantile\nalpha = 0.1  # 1 - alpha is the confidence level (e.g., 90% confidence)\nquantile = np.percentile(calibration_scores, (1 - alpha) * 100)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-22T02:35:18.250356Z","iopub.execute_input":"2025-03-22T02:35:18.250599Z","iopub.status.idle":"2025-03-22T02:35:18.265462Z","shell.execute_reply.started":"2025-03-22T02:35:18.250581Z","shell.execute_reply":"2025-03-22T02:35:18.26486Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"\nThe quantile is a value derived from the nonconformity scores on the calibration set.  It determines the width of the prediction intervals.  The `alpha` parameter represents our desired confidence level.  For example, if `alpha` is 0.1, we aim for 90% coverage.\n","metadata":{}},{"cell_type":"code","source":"# 7. Make Predictions and Construct Prediction Intervals\ny_pred = model.predict(X_test)\nprediction_intervals = []\nfor i, x in enumerate(X_test):\n    #print(x.shape) # (1,)\n    y_hat = y_pred[i]\n    interval_lower = y_hat - quantile\n    interval_upper = y_hat + quantile\n    prediction_intervals.append((interval_lower, interval_upper))","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-22T02:35:18.266207Z","iopub.execute_input":"2025-03-22T02:35:18.266575Z","iopub.status.idle":"2025-03-22T02:35:18.284666Z","shell.execute_reply.started":"2025-03-22T02:35:18.266548Z","shell.execute_reply":"2025-03-22T02:35:18.284032Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"\nFor each test point, we construct a prediction interval.  The interval is centered around the model's prediction and its width is determined by the quantile calculated from the calibration set.\n","metadata":{}},{"cell_type":"code","source":"# 8. Evaluate Coverage\ndef check_coverage(intervals, y_test):\n    covered = 0\n    for i, (lower, upper) in enumerate(intervals):\n        if lower <= y_test[i] <= upper:\n            covered += 1\n    return covered / len(y_test)\n\ncoverage = check_coverage(prediction_intervals, y_test)\nprint(f\"Coverage: {coverage * 100:.2f}%\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-22T02:35:18.286381Z","iopub.execute_input":"2025-03-22T02:35:18.286654Z","iopub.status.idle":"2025-03-22T02:35:18.304816Z","shell.execute_reply.started":"2025-03-22T02:35:18.286634Z","shell.execute_reply":"2025-03-22T02:35:18.303987Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"\nWe evaluate the coverage of our prediction intervals.  Coverage measures the proportion of test points for which the true value falls within the predicted interval.  Ideally, the coverage should be close to our target confidence level (1 - alpha).\n","metadata":{}},{"cell_type":"code","source":"# 9. Visualize Results\nplt.figure(figsize=(8, 6))\nplt.scatter(X, y, color='blue', label='Data')\nplt.plot(X_test, y_pred, color='red', label='Prediction')\nplt.fill_between(X_test.flatten(),\n                 [interval[0] for interval in prediction_intervals],\n                 [interval[1] for interval in prediction_intervals],\n                 color='gray', alpha=0.2, label='Prediction Interval')\nplt.xlabel('X')\nplt.ylabel('y')\nplt.legend()\nplt.title(f'Conformal Prediction (Coverage: {coverage * 100:.2f}%)')\nplt.show()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-22T02:35:18.306003Z","iopub.execute_input":"2025-03-22T02:35:18.306298Z","iopub.status.idle":"2025-03-22T02:35:18.540779Z","shell.execute_reply.started":"2025-03-22T02:35:18.306272Z","shell.execute_reply":"2025-03-22T02:35:18.539372Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"\nFinally, we visualize the results.  The plot shows the original data, the model's predictions, and the prediction intervals.  Visualizing the results helps to understand how conformal prediction quantifies the uncertainty of the predictions.\n","metadata":{}}]}