{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[],"dockerImageVersionId":30918,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"source":"<a href=\"https://www.kaggle.com/code/elainedazzio/20250405-cpb-p-values-and-nc-scores?scriptVersionId=261336181\" target=\"_blank\"><img align=\"left\" alt=\"Kaggle\" title=\"Open in Kaggle\" src=\"https://kaggle.com/static/images/open-in-kaggle.svg\"></a>","metadata":{},"cell_type":"markdown"},{"cell_type":"markdown","source":"# Conformal Prediction Basics: Understanding p-values and Nonconformity Scores\n\nThis notebook delves into the fundamental concepts of conformal prediction: p-values and nonconformity scores. We'll implement these concepts from scratch using Python and visualize how they're used to construct prediction sets with guaranteed coverage.  This builds on the basic conformal prediction example (https://www.kaggle.com/code/elainedazzio/simplest-conformal-prediction-example), and starts to address model misspecification (https://www.kaggle.com/code/elainedazzio/20250329-cpb-model-misspecification).\n\n**Key Concepts:**\n\n* **Nonconformity Score:** A measure of how \"strange\" a new data point is compared to the calibration data, *according to our model*.\n* **p-value:** The probability of observing a nonconformity score as large as, or larger than, the score of our test point, *under the assumption that the test point's label is correct*.\n* **Prediction Set:** A set of possible labels for a test point, constructed to guarantee a certain coverage probability (1 - alpha).\n\n**Goal:**\n\n1.  Implement nonconformity scores and p-value calculation from scratch.\n2.  Visualize the relationship between nonconformity scores, p-values, and prediction sets.\n3.  Understand how conformal prediction provides valid inference *regardless* of the underlying model's accuracy.\n\n","metadata":{}},{"cell_type":"code","source":"import numpy as np\nimport matplotlib.pyplot as plt\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.datasets import load_iris\nfrom typing import Callable, Optional, Tuple, List, Union","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-05T18:00:57.024152Z","iopub.execute_input":"2025-04-05T18:00:57.024654Z","iopub.status.idle":"2025-04-05T18:00:57.031647Z","shell.execute_reply.started":"2025-04-05T18:00:57.024532Z","shell.execute_reply":"2025-04-05T18:00:57.030129Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# --- Code Cell: Load and Preprocess Data ---\ndef load_and_split_data(test_size: float = 0.3, random_state: int = 42) -> Tuple[np.ndarray, np.ndarray, np.ndarray, np.ndarray, np.ndarray, np.ndarray, StandardScaler]:\n    iris = load_iris()\n    X, y = iris.data, iris.target\n\n    # Train-Calibration-Test split\n    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=test_size, random_state=random_state)\n    X_train, X_calibration, y_train, y_calibration = train_test_split(X_train, y_train, test_size=0.5, random_state=random_state)\n\n    # Feature scaling (important for many models)\n    scaler = StandardScaler()\n    X_train = scaler.fit_transform(X_train)\n    X_calibration = scaler.transform(X_calibration)\n    X_test = scaler.transform(X_test)\n\n    return X_train, X_calibration, X_test, y_train, y_calibration, y_test, scaler # Return the fitted scaler\n\n# Load data and get the scaler\nX_train, X_calibration, X_test, y_train, y_calibration, y_test, scaler = load_and_split_data()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-05T18:00:57.03321Z","iopub.execute_input":"2025-04-05T18:00:57.0335Z","iopub.status.idle":"2025-04-05T18:00:57.055432Z","shell.execute_reply.started":"2025-04-05T18:00:57.033473Z","shell.execute_reply":"2025-04-05T18:00:57.054396Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## 1. Nonconformity Score: Measuring Strangeness\n\nWe need a way to quantify how \"unusual\" a test point is compared to the calibration data. This is the role of the nonconformity score.  A higher nonconformity score means the point is considered more unusual.\n\nFor classification, a common nonconformity score is simply related to the probability assigned to the true class by our model.  We'll use:\n\n`Nonconformity Score = 1 - Probability(True Class)`\n\nThis score makes intuitive sense:\n\n* If the model is very confident in the correct class (probability close to 1), the nonconformity score is low (the point is \"conforming\").\n* If the model is unsure or assigns low probability to the correct class, the nonconformity score is high (the point is \"nonconforming\").\n","metadata":{}},{"cell_type":"code","source":"# --- Code Cell: Implement Nonconformity Score ---\ndef nonconformity_score(model: object, X: np.ndarray, y: np.ndarray) -> np.ndarray:\n    probabilities = model.predict_proba(X)\n    # Get the probability of the true class for each data point\n    true_class_probabilities = probabilities[np.arange(len(y)), y]\n    # Nonconformity score: 1 - probability of the true class\n    return 1 - true_class_probabilities","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-05T18:00:57.057525Z","iopub.execute_input":"2025-04-05T18:00:57.057906Z","iopub.status.idle":"2025-04-05T18:00:57.070937Z","shell.execute_reply.started":"2025-04-05T18:00:57.05788Z","shell.execute_reply":"2025-04-05T18:00:57.069646Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## 2. Train a Classification Model\n\nWe'll use a simple Logistic Regression model for our example.  Conformal prediction works with *any* model, but we need one to generate probabilities.\n","metadata":{}},{"cell_type":"code","source":"# --- Code Cell: Train Logistic Regression ---\n# Train a logistic regression model\nmodel = LogisticRegression(random_state=42)\nmodel.fit(X_train, y_train)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-05T18:00:57.072509Z","iopub.execute_input":"2025-04-05T18:00:57.072829Z","iopub.status.idle":"2025-04-05T18:00:57.100549Z","shell.execute_reply.started":"2025-04-05T18:00:57.072804Z","shell.execute_reply":"2025-04-05T18:00:57.099408Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"\n\n## 3. Calculate Nonconformity Scores on the Calibration Set\n\nNow, let's calculate the nonconformity scores for our calibration data.  These scores are crucial; they form the basis for determining the threshold for our prediction sets.\n","metadata":{}},{"cell_type":"code","source":"# --- Code Cell: Calculate Calibration Scores ---\n# Calculate nonconformity scores for the calibration set\ncalibration_scores = nonconformity_score(model, X_calibration, y_calibration)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-05T18:00:57.101936Z","iopub.execute_input":"2025-04-05T18:00:57.102396Z","iopub.status.idle":"2025-04-05T18:00:57.113849Z","shell.execute_reply.started":"2025-04-05T18:00:57.102356Z","shell.execute_reply":"2025-04-05T18:00:57.112731Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## 4. The Empirical p-value: Quantifying Strangeness for a New Point\n\nThe p-value measures how \"strange\" a new test point is *relative* to the calibration data.\n\nFor a new test point, we calculate its nonconformity score.  The p-value is the proportion of calibration points with a nonconformity score *greater than or equal to* the test point's score.\n\nMathematically:\n\n`p-value(x_test, y_potential) = (Number of calibration points with score >= nonconformity_score(x_test, y_potential) + 1) / (Number of calibration points + 1)`\n\nThe \"+ 1\" in the numerator and denominator ensures that the p-value is always defined, even if the test point's nonconformity score is higher than all calibration scores.\n","metadata":{}},{"cell_type":"code","source":"# --- Code Cell:  Implement p-value Calculation ---\ndef calculate_p_value(test_score: float, calibration_scores: np.ndarray) -> float:\n    # Number of calibration scores greater than or equal to the test score\n    greater_equal_scores = np.sum(calibration_scores >= test_score)\n    # Empirical p-value\n    return (greater_equal_scores + 1) / (len(calibration_scores) + 1)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-05T18:00:57.11526Z","iopub.execute_input":"2025-04-05T18:00:57.115727Z","iopub.status.idle":"2025-04-05T18:00:57.136764Z","shell.execute_reply.started":"2025-04-05T18:00:57.115689Z","shell.execute_reply":"2025-04-05T18:00:57.135626Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## 5. Constructing Prediction Sets\n\nGiven a desired confidence level (1 - alpha), we form a prediction set for a test point as follows:\n\nFor each possible label `y_potential`:\n\n1.  Calculate the nonconformity score of the test point *assuming* `y_potential` is the correct label.\n2.  Calculate the p-value using the calibration scores.\n3.  Include `y_potential` in the prediction set if its p-value is greater than or equal to `alpha`.\n\nThe prediction set is the collection of all `y_potential` values that satisfy this condition.\n","metadata":{}},{"cell_type":"code","source":"# --- Code Cell:  Construct Prediction Set ---\ndef create_prediction_set(model: object, X_test: np.ndarray, y_labels: np.ndarray, calibration_scores: np.ndarray, alpha: float = 0.1) -> List[List[int]]:\n    prediction_sets = []\n    for x_test in X_test:\n        # Initialize an empty set for this test point\n        current_prediction_set = []\n        for y_potential in y_labels:\n            # Need to reshape x_test to (1, n_features) for the nonconformity_score function\n            x_test_reshaped = x_test.reshape(1, -1)\n            # Calculate the nonconformity score assuming y_potential is the true label\n            test_score = nonconformity_score(model, x_test_reshaped, np.array([y_potential]))[0]\n            # Calculate the p-value\n            p_value = calculate_p_value(test_score, calibration_scores)\n            # Include the label in the prediction set if the p-value is >= alpha\n            if p_value >= alpha:\n                current_prediction_set.append(y_potential)\n        prediction_sets.append(current_prediction_set)\n    return prediction_sets\n\n# Get all possible labels from the training data\ny_labels = np.unique(y_train)\n# Create prediction sets for the test data\nalpha = 0.1  # 90% coverage\nprediction_sets = create_prediction_set(model, X_test, y_labels, calibration_scores, alpha)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-05T18:00:57.137889Z","iopub.execute_input":"2025-04-05T18:00:57.138254Z","iopub.status.idle":"2025-04-05T18:00:57.174291Z","shell.execute_reply.started":"2025-04-05T18:00:57.138217Z","shell.execute_reply":"2025-04-05T18:00:57.173256Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## 6. Evaluating Coverage\n\nA key property of conformal prediction is that it guarantees a certain level of coverage.  Specifically, for a given `alpha`, the true label should be in the prediction set at least `(1 - alpha) * 100%` of the time.\n\nLet's check if our implementation achieves this guarantee.\n","metadata":{}},{"cell_type":"code","source":"# --- Code Cell:  Evaluate Coverage ---\ndef evaluate_coverage(prediction_sets: List[List[int]], y_test: np.ndarray) -> float:\n    correct_predictions = 0\n    for i, y_true in enumerate(y_test):\n        if y_true in prediction_sets[i]:\n            correct_predictions += 1\n    return correct_predictions / len(y_test)\n\ncoverage = evaluate_coverage(prediction_sets, y_test)\nprint(f\"Coverage: {coverage:.3f}\")\nprint(f\"Desired coverage: {1 - alpha}\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-05T18:00:57.175761Z","iopub.execute_input":"2025-04-05T18:00:57.176185Z","iopub.status.idle":"2025-04-05T18:00:57.197546Z","shell.execute_reply.started":"2025-04-05T18:00:57.176143Z","shell.execute_reply":"2025-04-05T18:00:57.196374Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## 7. Visualizing p-value Distribution (Optional)\n\nIt can be helpful to visualize the distribution of p-values, especially for the *correct* labels.  Ideally, these p-values should be roughly uniform between 0 and 1.  Deviations from uniformity can sometimes indicate issues with the model or the exchangeability assumption.\n","metadata":{}},{"cell_type":"code","source":"# --- Code Cell: Visualize p-value Distribution ---\ndef plot_p_value_distribution(model: object, X_test: np.ndarray, y_test: np.ndarray, calibration_scores: np.ndarray) -> None:\n    p_values = []\n    for i, x_test in enumerate(X_test):\n        x_test_reshaped = x_test.reshape(1, -1)\n        # Calculate the nonconformity score for the true label\n        test_score = nonconformity_score(model, x_test_reshaped, np.array([y_test[i]]))[0]\n        p_value = calculate_p_value(test_score, calibration_scores)\n        p_values.append(p_value)\n\n    plt.hist(p_values, bins=20, alpha=0.7)\n    plt.xlabel(\"p-value\")\n    plt.ylabel(\"Frequency\")\n    plt.title(\"Distribution of p-values for True Labels\")\n    plt.show()\n\nplot_p_value_distribution(model, X_test, y_test, calibration_scores)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-05T18:00:57.199936Z","iopub.execute_input":"2025-04-05T18:00:57.200289Z","iopub.status.idle":"2025-04-05T18:00:57.448469Z","shell.execute_reply.started":"2025-04-05T18:00:57.200262Z","shell.execute_reply":"2025-04-05T18:00:57.447377Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## 8. Example Prediction Sets\n\nLet's look at a few examples of the prediction sets generated by our conformal predictor.\n","metadata":{}},{"cell_type":"code","source":"# --- Code Cell: Print Example Prediction Sets ---\n\ndef print_example_prediction_sets(prediction_sets: List[List[int]], X_test_scaled: np.ndarray, scaler: StandardScaler, y_test: np.ndarray, target_names: List[str], num_examples: int = 5) -> None:\n    print(\"Example Prediction Sets:\")\n    # Get original feature values by inverting the scaling for the examples we want to show\n    X_test_original = scaler.inverse_transform(X_test_scaled[:num_examples])\n\n    for i in range(min(num_examples, len(prediction_sets))):\n        original_features = X_test_original[i] # Use the inverse transformed features\n        feature_str = \", \".join([f\"{name}: {value:.2f}\" for name, value in zip(load_iris().feature_names, original_features)])\n        print(f\"Test point {i+1} - Features: {feature_str}, True Label: {target_names[y_test[i]]}, Prediction Set: {[target_names[label] for label in prediction_sets[i]]}\")\n\n# Get target names for Iris dataset.\ntarget_names = load_iris().target_names.tolist()\n# Pass the scaled X_test and the fitted scaler to the function\nprint_example_prediction_sets(prediction_sets, X_test, scaler, y_test, target_names)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-05T18:00:57.449544Z","iopub.execute_input":"2025-04-05T18:00:57.449837Z","iopub.status.idle":"2025-04-05T18:00:57.465031Z","shell.execute_reply.started":"2025-04-05T18:00:57.449808Z","shell.execute_reply":"2025-04-05T18:00:57.464098Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## Discussion and Conclusion\n\nThis notebook demonstrated the core mechanics of conformal prediction:\n\n1.  We calculated nonconformity scores to measure the \"strangeness\" of data points.\n2.  We used these scores to compute empirical p-values, quantifying the likelihood of observing a score as extreme as the test point's score.\n3.  We constructed prediction sets by including all labels whose p-values met our desired confidence level (1 - alpha).\n\nKey Takeaways:\n\n* Conformal prediction provides *guaranteed coverage* regardless of the underlying model's accuracy.  The calibration data informs us about the model's errors.\n* The choice of nonconformity score is crucial.  It should reflect how well the model's prediction aligns with the observed data.\n* The p-value is a fundamental quantity that allows us to convert nonconformity into a probabilistic statement about the plausibility of different labels.\n\nNext Steps:\n\n* Experiment with different nonconformity scores.\n* Explore how the choice of `alpha` affects the size and coverage of the prediction sets.\n* Investigate conformal prediction with different machine learning models.\n","metadata":{}}]}