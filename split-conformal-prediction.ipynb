{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "814f5514",
   "metadata": {},
   "source": [
    "<a href=\"https://colab.research.google.com/github/LEDazzio01/Simplest-Conformal-Prediction/blob/main/split-conformal-prediction.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eee0549f",
   "metadata": {},
   "source": [
    "# Split Conformal Prediction\n",
    "\n",
    "This notebook demonstrates **split conformal prediction**â€”the standard, correct way to construct prediction intervals with guaranteed coverage.\n",
    "\n",
    "## What is Split Conformal Prediction?\n",
    "\n",
    "Split conformal prediction divides your data into:\n",
    "1. **Training set**: Used to fit the model\n",
    "2. **Calibration set**: Used to compute nonconformity scores (model never sees this during training)\n",
    "3. **Test set**: Used to evaluate coverage\n",
    "\n",
    "## Why \"Split\"?\n",
    "\n",
    "The key insight: **calibration scores must come from data the model hasn't seen**. This ensures:\n",
    "- Unbiased error estimates\n",
    "- Valid coverage guarantees\n",
    "- Intervals that aren't overconfident\n",
    "\n",
    "## What You'll Learn\n",
    "\n",
    "1. How to properly split data for conformal prediction\n",
    "2. Why calibration data must be separate from training data\n",
    "3. How to verify coverage on held-out test data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a26c6e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- 1. Import Libraries ---\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LinearRegression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad63475e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- 2. Generate Synthetic Data ---\n",
    "\n",
    "np.random.seed(42)\n",
    "\n",
    "n_samples = 300\n",
    "\n",
    "# Generate X values uniformly between 0 and 10\n",
    "X = np.random.uniform(0, 10, n_samples).reshape(-1, 1)\n",
    "\n",
    "# True relationship: y = 2x + 1 + noise\n",
    "y = 2 * X.flatten() + 1 + np.random.normal(0, 2, n_samples)\n",
    "\n",
    "print(f\"Total samples: {n_samples}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12930dc0",
   "metadata": {},
   "source": [
    "### About the Data\n",
    "\n",
    "We generated synthetic regression data:\n",
    "- **True relationship**: `y = 2x + 1`\n",
    "- **Noise**: Gaussian with Ïƒ = 2\n",
    "- **300 samples**: Enough to split into three meaningful sets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb3e9869",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- 3. Split Data: Train / Calibration / Test ---\n",
    "\n",
    "# First split: 60% train, 40% held out\n",
    "X_train, X_temp, y_train, y_temp = train_test_split(\n",
    "    X, y, test_size=0.4, random_state=42\n",
    ")\n",
    "\n",
    "# Second split: held out â†’ 50% calibration, 50% test\n",
    "X_cal, X_test, y_cal, y_test = train_test_split(\n",
    "    X_temp, y_temp, test_size=0.5, random_state=42\n",
    ")\n",
    "\n",
    "print(f\"Training set:    {len(X_train)} samples (60%)\")\n",
    "print(f\"Calibration set: {len(X_cal)} samples (20%)\")\n",
    "print(f\"Test set:        {len(X_test)} samples (20%)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d4b65a4b",
   "metadata": {},
   "source": [
    "### The Three-Way Split\n",
    "\n",
    "| Set | Size | Purpose |\n",
    "|-----|------|---------|\n",
    "| **Training** | 60% | Fit the model |\n",
    "| **Calibration** | 20% | Compute nonconformity scores |\n",
    "| **Test** | 20% | Evaluate coverage |\n",
    "\n",
    "### âš ï¸ Critical Rule\n",
    "\n",
    "> The model must **never** see the calibration data during training.\n",
    "\n",
    "This separation ensures our nonconformity scores reflect true out-of-sample errors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1007a55c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- 4. Train the Model (on training data ONLY) ---\n",
    "\n",
    "model = LinearRegression()\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "print(f\"Fitted model: y = {model.coef_[0]:.3f}x + {model.intercept_:.3f}\")\n",
    "print(f\"(True model:  y = 2.000x + 1.000)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32eb4380",
   "metadata": {},
   "source": [
    "### Model Training\n",
    "\n",
    "We train **only on the training set**. The model has never seen:\n",
    "- Calibration data (used for computing scores)\n",
    "- Test data (used for evaluation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e23cb50f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- 5. Compute Nonconformity Scores on Calibration Set ---\n",
    "\n",
    "# Predict on calibration set (data model hasn't seen)\n",
    "y_cal_pred = model.predict(X_cal)\n",
    "\n",
    "# Nonconformity scores = absolute residuals\n",
    "cal_scores = np.abs(y_cal - y_cal_pred)\n",
    "\n",
    "print(f\"Calibration scores:\")\n",
    "print(f\"  Min:    {cal_scores.min():.3f}\")\n",
    "print(f\"  Max:    {cal_scores.max():.3f}\")\n",
    "print(f\"  Mean:   {cal_scores.mean():.3f}\")\n",
    "print(f\"  Median: {np.median(cal_scores):.3f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "95ebb7c2",
   "metadata": {},
   "source": [
    "### Nonconformity Scores\n",
    "\n",
    "We compute absolute residuals on the **calibration set**:\n",
    "\n",
    "$$\\text{score}_i = |y_i - \\hat{y}_i|$$\n",
    "\n",
    "These scores represent the model's typical errors on unseen dataâ€”crucial for setting the right interval width."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8a8254a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- 6. Calculate the Quantile Threshold ---\n",
    "\n",
    "alpha = 0.1  # Target: 90% coverage\n",
    "\n",
    "# Finite-sample correction for coverage guarantee\n",
    "n_cal = len(cal_scores)\n",
    "q_level = np.ceil((n_cal + 1) * (1 - alpha)) / n_cal\n",
    "q_level = min(q_level, 1.0)  # Cap at 1.0\n",
    "\n",
    "# Find the quantile\n",
    "quantile = np.quantile(cal_scores, q_level)\n",
    "\n",
    "print(f\"Target coverage: {(1 - alpha) * 100:.0f}%\")\n",
    "print(f\"Adjusted quantile level: {q_level:.3f}\")\n",
    "print(f\"Quantile threshold: {quantile:.3f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6fd6229d",
   "metadata": {},
   "source": [
    "### The Quantile Threshold\n",
    "\n",
    "We use a **finite-sample correction** to ensure valid coverage:\n",
    "\n",
    "$$q = \\frac{\\lceil (n+1)(1-\\alpha) \\rceil}{n}$$\n",
    "\n",
    "This adjustment accounts for the finite size of the calibration set and guarantees coverage of at least $(1-\\alpha)$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4bf9f61a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- 7. Construct Prediction Intervals on Test Set ---\n",
    "\n",
    "# Get predictions for test set\n",
    "y_test_pred = model.predict(X_test)\n",
    "\n",
    "# Construct intervals\n",
    "lower_bounds = y_test_pred - quantile\n",
    "upper_bounds = y_test_pred + quantile\n",
    "\n",
    "print(f\"Interval width: Â±{quantile:.3f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "63e1fb1b",
   "metadata": {},
   "source": [
    "### Prediction Intervals\n",
    "\n",
    "For each test point:\n",
    "\n",
    "$$\\text{Interval} = [\\hat{y} - q, \\quad \\hat{y} + q]$$\n",
    "\n",
    "Where:\n",
    "- $\\hat{y}$ = model prediction\n",
    "- $q$ = quantile threshold from calibration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f20f31b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- 8. Evaluate Coverage on Test Set ---\n",
    "\n",
    "# Check which test points are covered\n",
    "covered = (y_test >= lower_bounds) & (y_test <= upper_bounds)\n",
    "coverage = np.mean(covered)\n",
    "\n",
    "print(f\"Target coverage:  {(1 - alpha) * 100:.0f}%\")\n",
    "print(f\"Actual coverage:  {coverage * 100:.1f}%\")\n",
    "print(f\"Difference:       {(coverage - (1 - alpha)) * 100:+.1f}%\")\n",
    "print(f\"\\nCovered: {covered.sum()} / {len(y_test)} test points\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d822921",
   "metadata": {},
   "source": [
    "### Coverage Evaluation\n",
    "\n",
    "**Coverage** = proportion of test points where the true value falls within the interval.\n",
    "\n",
    "With split conformal prediction:\n",
    "- Coverage should be **at least** $(1 - \\alpha)$\n",
    "- Small overcoverage is normal (conservative is okay!)\n",
    "- Undercoverage would indicate a problem"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "571fad40",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- 9. Visualize Results ---\n",
    "\n",
    "fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
    "\n",
    "# --- Left plot: All data with intervals ---\n",
    "ax1 = axes[0]\n",
    "\n",
    "# Plot training data\n",
    "ax1.scatter(X_train, y_train, alpha=0.3, s=20, label='Training Data', color='blue')\n",
    "\n",
    "# Plot calibration data\n",
    "ax1.scatter(X_cal, y_cal, alpha=0.5, s=30, label='Calibration Data', color='orange')\n",
    "\n",
    "# Plot test data\n",
    "ax1.scatter(X_test, y_test, alpha=0.7, s=40, label='Test Data', color='green')\n",
    "\n",
    "# Plot model line\n",
    "X_line = np.linspace(0, 10, 100).reshape(-1, 1)\n",
    "y_line = model.predict(X_line)\n",
    "ax1.plot(X_line, y_line, 'r-', linewidth=2, label='Model')\n",
    "\n",
    "ax1.set_xlabel('X', fontsize=11)\n",
    "ax1.set_ylabel('y', fontsize=11)\n",
    "ax1.set_title('Data Split: Train / Calibration / Test', fontsize=12)\n",
    "ax1.legend(loc='upper left')\n",
    "ax1.grid(alpha=0.3)\n",
    "\n",
    "# --- Right plot: Test predictions with intervals ---\n",
    "ax2 = axes[1]\n",
    "\n",
    "# Sort for cleaner visualization\n",
    "sort_idx = np.argsort(X_test.flatten())\n",
    "X_sorted = X_test.flatten()[sort_idx]\n",
    "y_test_sorted = y_test[sort_idx]\n",
    "pred_sorted = y_test_pred[sort_idx]\n",
    "lower_sorted = lower_bounds[sort_idx]\n",
    "upper_sorted = upper_bounds[sort_idx]\n",
    "covered_sorted = covered[sort_idx]\n",
    "\n",
    "# Plot prediction intervals\n",
    "ax2.fill_between(X_sorted, lower_sorted, upper_sorted, \n",
    "                 alpha=0.3, color='blue', label=f'{(1-alpha)*100:.0f}% Prediction Interval')\n",
    "\n",
    "# Plot predictions\n",
    "ax2.plot(X_sorted, pred_sorted, 'r-', linewidth=2, label='Predictions')\n",
    "\n",
    "# Plot test points (colored by coverage)\n",
    "ax2.scatter(X_sorted[covered_sorted], y_test_sorted[covered_sorted], \n",
    "            color='green', s=50, zorder=5, label=f'Covered ({covered.sum()})')\n",
    "ax2.scatter(X_sorted[~covered_sorted], y_test_sorted[~covered_sorted], \n",
    "            color='red', s=50, zorder=5, marker='x', label=f'Not covered ({(~covered).sum()})')\n",
    "\n",
    "ax2.set_xlabel('X', fontsize=11)\n",
    "ax2.set_ylabel('y', fontsize=11)\n",
    "ax2.set_title(f'Test Set Coverage: {coverage*100:.1f}% (Target: {(1-alpha)*100:.0f}%)', fontsize=12)\n",
    "ax2.legend(loc='upper left')\n",
    "ax2.grid(alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a91cc0b",
   "metadata": {},
   "source": [
    "### Visualization Summary\n",
    "\n",
    "**Left plot**: Shows how data is split into three sets\n",
    "- Blue: Training data (model fitted here)\n",
    "- Orange: Calibration data (scores computed here)\n",
    "- Green: Test data (coverage evaluated here)\n",
    "\n",
    "**Right plot**: Prediction intervals on test set\n",
    "- Blue band: 90% prediction intervals\n",
    "- Green dots: Test points inside intervals (covered)\n",
    "- Red X's: Test points outside intervals (not covered)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef117ec6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- 10. Compare with Naive Approach (Using Training Data for Calibration) ---\n",
    "\n",
    "# WRONG: Using training data for calibration\n",
    "y_train_pred = model.predict(X_train)\n",
    "train_scores = np.abs(y_train - y_train_pred)\n",
    "naive_quantile = np.quantile(train_scores, 1 - alpha)\n",
    "\n",
    "# Construct intervals with naive approach\n",
    "naive_lower = y_test_pred - naive_quantile\n",
    "naive_upper = y_test_pred + naive_quantile\n",
    "\n",
    "# Check coverage\n",
    "naive_covered = (y_test >= naive_lower) & (y_test <= naive_upper)\n",
    "naive_coverage = np.mean(naive_covered)\n",
    "\n",
    "print(\"Comparison: Proper vs Naive Calibration\")\n",
    "print(\"=\" * 45)\n",
    "print(f\"{'Method':<25} {'Interval Width':<15} {'Coverage':<10}\")\n",
    "print(\"-\" * 45)\n",
    "print(f\"{'Split (proper)':<25} Â±{quantile:<14.3f} {coverage*100:.1f}%\")\n",
    "print(f\"{'Naive (training data)':<25} Â±{naive_quantile:<14.3f} {naive_coverage*100:.1f}%\")\n",
    "print(\"-\" * 45)\n",
    "print(f\"\\nNaive intervals are {(quantile - naive_quantile)/quantile*100:.1f}% narrower\")\n",
    "print(f\"But coverage drops by {(coverage - naive_coverage)*100:.1f} percentage points!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e135606",
   "metadata": {},
   "source": [
    "### Why Split Matters\n",
    "\n",
    "The comparison shows:\n",
    "\n",
    "| Method | Interval Width | Coverage |\n",
    "|--------|---------------|----------|\n",
    "| **Split (proper)** | Wider | âœ… Achieves target |\n",
    "| **Naive (training data)** | Narrower | âŒ Under-covers |\n",
    "\n",
    "Using training data for calibration produces **overconfident** intervals because:\n",
    "- Training errors are optimistically biased\n",
    "- The model was fitted to minimize these exact errors\n",
    "- Out-of-sample errors are typically larger\n",
    "\n",
    "---\n",
    "\n",
    "## ðŸ”‘ Key Takeaways\n",
    "\n",
    "1. **Always use a separate calibration set** for computing nonconformity scores\n",
    "2. **Never use training data for calibration**â€”it produces overconfident intervals\n",
    "3. **Split conformal prediction** provides valid coverage guarantees\n",
    "4. The finite-sample correction ensures coverage even with small calibration sets\n",
    "\n",
    "## The Split Conformal Recipe\n",
    "\n",
    "```\n",
    "1. Split data â†’ Training / Calibration / Test\n",
    "2. Train model on Training set only\n",
    "3. Compute scores on Calibration set\n",
    "4. Find quantile with finite-sample correction\n",
    "5. Construct intervals: prediction Â± quantile\n",
    "6. Evaluate coverage on Test set\n",
    "```"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
